{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# non gene examples: 19800\n",
      "# gene examples: 19425\n",
      "Bases in non gene: {'C', 'T', 'G', 'A'}\n",
      "Bases in gene: {'C', 'T', 'G', 'A'}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with open('data/non_gene_seqs.pkl', 'rb') as f:\n",
    "    non_gene = pickle.load(f)\n",
    "with open('data/gene_seqs.pkl', 'rb') as f:\n",
    "    gene = pickle.load(f)\n",
    "\n",
    "# Check class balance\n",
    "print(\"# non gene examples: {}\".format(len(non_gene)))\n",
    "print(\"# gene examples: {}\".format(len(gene)))\n",
    "    \n",
    "# Check for weird bases\n",
    "print(\"Bases in non gene: {}\".format(set(\"\".join(non_gene))))\n",
    "print(\"Bases in gene: {}\".format(set(\"\".join(gene))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:35302, test:7845, validation:3923\n"
     ]
    }
   ],
   "source": [
    "# Form test, train and validation sets (class 0 is non-gene and class 1 is gene)\n",
    "df_non_gene = pd.DataFrame.from_dict({'sequence':non_gene, 'class': 0})\n",
    "df_gene = pd.DataFrame.from_dict({'sequence':gene, 'class': 1})\n",
    "df = pd.concat([df_non_gene, df_gene])\n",
    "df = df.sample(frac=1)  # randomly shuffle dataframe rows\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(df, test_size=0.1)\n",
    "print(\"Train:{}, test:{}, validation:{}\".format(len(train), len(test), len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to test and train matrices\n",
    "word_dict = dict(zip(['A', 'C', 'G', 'T'], list(range(4))))\n",
    "\n",
    "def gene_seq_to_vec(seq, word_dict):\n",
    "    for key in word_dict:\n",
    "        seq = seq.replace(key, str(word_dict[key]))\n",
    "    return list(map(int, list(seq)))\n",
    "\n",
    "X_train = np.array([gene_seq_to_vec(x, word_dict) for x in train['sequence'].tolist()])\n",
    "X_test = np.array([gene_seq_to_vec(x, word_dict) for x in test['sequence'].tolist()])\n",
    "X_val = np.array([gene_seq_to_vec(x, word_dict) for x in val['sequence'].tolist()])\n",
    "\n",
    "y_train = np.array(train['class'].tolist()).reshape(-1, 1)\n",
    "y_test = np.array(test['class'].tolist()).reshape(-1, 1)\n",
    "y_val = np.array(val['class'].tolist()).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize TF graph and set training paramaters\n",
    "tf.reset_default_graph()\n",
    "cell_units = 128\n",
    "n_hidden = 64\n",
    "learning_rate = 0.01\n",
    "b_size = 100\n",
    "n_epochs = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1000, 1])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "def gru_rnn(x, y, cell_units, n_hidden, learning_rate):\n",
    "    \"\"\"Build TF computation graph for RNN\"\"\"\n",
    "    W1 = tf.get_variable(\"W1\", [cell_units, n_hidden],\n",
    "                         initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "    W2 = tf.get_variable(\"W2\", [n_hidden, 1],\n",
    "                         initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "    b1 = tf.get_variable(\"b1\", [n_hidden], \n",
    "                         initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "    b2 = tf.get_variable(\"b2\", [1], \n",
    "                         initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))\n",
    "    #Â LSTM layer\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=cell_units)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell=lstm, dtype=tf.float32, inputs=x)\n",
    "\n",
    "    # Linear layer + ReLU + linear layer\n",
    "    final_output = outputs[:, -1, :]\n",
    "    h1 = tf.nn.relu(tf.matmul(final_output, W1) + b1)\n",
    "    h2 = tf.matmul(h1, W2) + b2\n",
    "    output = tf.nn.sigmoid(h2)\n",
    "\n",
    "    # Predict on final output and calculate loss function\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=h2))\n",
    "    acc = tf.reduce_sum(tf.cast(tf.equal(tf.round(output), y), 'float'))\n",
    "\n",
    "    # Train RNN\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    return output, cost, opt, acc\n",
    "\n",
    "output, cost, opt, acc = gru_rnn(x, y, cell_units, n_hidden, learning_rate)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------- Running for epoch 1 of 1\n",
      "Trained for 0 batches - loss = 0.6933265328407288, accuracy = 0.46\n",
      "Trained for 10 batches - loss = 0.6932650804519653, accuracy = 0.46\n",
      "Trained for 20 batches - loss = 0.6910545229911804, accuracy = 0.57\n",
      "Trained for 30 batches - loss = 0.6940783858299255, accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Split into batches and train\n",
    "X_train_batch_list = np.array_split(X_train, int(len(train)/b_size))\n",
    "y_train_batch_list = np.array_split(y_train, int(len(train)/b_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"#------- Running for epoch {} of {}\".format(epoch+1, n_epochs))\n",
    "        t0 = time.time()\n",
    "        for i in range(len(X_train_batch_list)):\n",
    "            train_dict = {\n",
    "                x: X_train_batch_list[i].reshape(-1, 1000, 1),\n",
    "                y: y_train_batch_list[i]\n",
    "            }\n",
    "            c, o, a = sess.run([cost, opt, acc], feed_dict=train_dict)\n",
    "            if i % 10 == 0:\n",
    "                print('Trained for {} batches - loss = {}, accuracy = {}'.format(i, c, (a/b_size)))\n",
    "                \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
